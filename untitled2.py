# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hZZYif5ig35KptisGx-F_6-wIiPeWwxq
"""

!pip install grad-cam
!pip install torchmetrics

# --------- 1) KAGGLE DOWNLOAD (HAM10000) ---------
# Upload kaggle.json once when prompted

from google.colab import files
import os, zipfile, glob
import pandas as pd

# Step 1: Upload kaggle.json
if not os.path.exists("/root/.kaggle/kaggle.json"):
    print("ðŸ‘‰ Please upload kaggle.json (from your Kaggle account settings).")
    uploaded = files.upload()  # upload kaggle.json
    os.makedirs("/root/.kaggle", exist_ok=True)
    for k, v in uploaded.items():
        with open("/root/.kaggle/kaggle.json", "wb") as f:
            f.write(v)
    os.chmod("/root/.kaggle/kaggle.json", 0o600)

# Step 2: Install Kaggle CLI
!pip -q install kaggle

# Step 3: Download HAM10000 dataset
DATA_DIR = "/content/ham10000"
os.makedirs(DATA_DIR, exist_ok=True)
!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000 -p "{DATA_DIR}" -o

# Step 4: Unzip everything
zip_files = glob.glob(f"{DATA_DIR}/*.zip")
for z in zip_files:
    with zipfile.ZipFile(z, "r") as zip_ref:
        zip_ref.extractall(DATA_DIR)

print("âœ… Dataset downloaded and extracted!")
print("Files in DATA_DIR:", os.listdir(DATA_DIR))

# Step 5: Fix folder names if needed
IMG_DIRS = [os.path.join(DATA_DIR, "ham10000_images_part_1"),
            os.path.join(DATA_DIR, "ham10000_images_part_2")]
for d in IMG_DIRS:
    if not os.path.exists(d):
        alt = os.path.join(DATA_DIR, d.split("/")[-1].upper())
        if os.path.exists(alt):
            os.rename(alt, d)

# Step 6: Load metadata
META_CSV = os.path.join(DATA_DIR, "HAM10000_metadata.csv")
assert os.path.exists(META_CSV), "HAM10000_metadata.csv not found!"
meta = pd.read_csv(META_CSV)
print("Metadata preview:")
print(meta.head())

import os, time

ARTIFACTS_DIR = f"/content/drive/MyDrive/ham10000_mm_robust/{time.strftime('%Y%m%d_%H%M%S')}"
os.makedirs(ARTIFACTS_DIR, exist_ok=True)
print("All artifacts will be saved under:", ARTIFACTS_DIR)

# --------- 2) TRANSFORMS + DATALOADERS ---------

import shutil

old_split = "/content/drive/MyDrive/ham10000_mm_robust/20250820_051459/splits.json"
new_split = "/content/drive/MyDrive/ham10000_mm_robust/20250820_052510/splits.json"

shutil.copy(old_split, new_split)
print("Copied splits.json to new ARTIFACTS_DIR")


import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2, os, json

# Image size (height, width)
IMG_SIZE = 384

# âœ… Corrected transforms (Albumentations requires tuple (H,W))
train_tfms = A.Compose([
    A.Resize(IMG_SIZE, IMG_SIZE),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.Normalize(),
    ToTensorV2()
])

valid_tfms = A.Compose([
    A.Resize(IMG_SIZE, IMG_SIZE),
    A.Normalize(),
    ToTensorV2()
])

# --------- Custom Dataset ---------
class HAM10000Dataset(Dataset):
    def __init__(self, df, img_dirs, tfms=None, label2idx=None):
        self.df = df.reset_index(drop=True)
        self.img_dirs = img_dirs
        self.tfms = tfms
        self.label2idx = label2idx if label2idx else {l:i for i,l in enumerate(df['dx'].unique())}

    def __len__(self): return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_id, label = row['image_id'], row['dx']
        # find image
        for d in self.img_dirs:
            img_path = os.path.join(d, f"{img_id}.jpg")
            if os.path.exists(img_path):
                break
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        if self.tfms:
            image = self.tfms(image=image)["image"]
        return image, self.label2idx[label]

# --------- Load saved splits ---------
splits_path = os.path.join(ARTIFACTS_DIR, "splits.json")
with open(splits_path, "r") as f:
    splits = json.load(f)
train_df = meta[meta["image_id"].isin(splits["train"])]
valid_df = meta[meta["image_id"].isin(splits["val"])]  # fix here
test_df  = meta[meta["image_id"].isin(splits["test"])]

label2idx = {l:i for i,l in enumerate(sorted(meta["dx"].unique()))}
idx2label = {i:l for l,i in label2idx.items()}

# --------- Create datasets ---------
train_ds = HAM10000Dataset(train_df, IMG_DIRS, tfms=train_tfms, label2idx=label2idx)
valid_ds = HAM10000Dataset(valid_df, IMG_DIRS, tfms=valid_tfms, label2idx=label2idx)
test_ds  = HAM10000Dataset(test_df,  IMG_DIRS, tfms=valid_tfms, label2idx=label2idx)

# --------- Create dataloaders ---------
BATCH_SIZE = 32

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

print(f"Train batches: {len(train_loader)} | Valid batches: {len(valid_loader)} | Test batches: {len(test_loader)}")

# --------- Quick sanity check ---------
images, labels = next(iter(train_loader))
print("Batch shape:", images.shape, "Labels:", labels[:10])

# --------- 3) MODEL + TRAINING ---------
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
from tqdm import tqdm
import os

# Number of classes
NUM_CLASSES = len(label2idx)

# --------- Load Pretrained ResNet50 ---------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

model = models.resnet50(weights="IMAGENET1K_V1")  # pretrained
in_features = model.fc.in_features
model.fc = nn.Linear(in_features, NUM_CLASSES)   # replace final layer
model = model.to(device)

# --------- Loss + Optimizer + Scheduler ---------
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

# --------- Training Loop ---------
def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    running_loss, correct, total = 0, 0, 0
    for imgs, labels in tqdm(loader, leave=False):
        imgs, labels = imgs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * imgs.size(0)
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total

def eval_one_epoch(model, loader, criterion):
    model.eval()
    running_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * imgs.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return running_loss / total, correct / total

# --------- Train and Save Best ---------
EPOCHS = 10
best_acc = 0.0
save_path = os.path.join(ARTIFACTS_DIR, "resnet50_best.pth")

for epoch in range(EPOCHS):
    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)
    val_loss, val_acc = eval_one_epoch(model, valid_loader, criterion)
    scheduler.step()

    print(f"Epoch {epoch+1}/{EPOCHS} | "
          f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | "
          f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")

    # Save best model
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save({
            'epoch': epoch+1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': best_acc
        }, save_path)
        print(f"âœ… Saved new best model at epoch {epoch+1} with acc {best_acc:.4f}")

print(f"Training finished. Best Val Acc: {best_acc:.4f}")

# ============================================
# PART 4: Model Definition
# ============================================
import torch
import torch.nn as nn
import torchvision.models as models
import os

print("Device:", device)

# --------- Define Model ---------
def get_model(num_classes=7, pretrained=True):
    model = models.resnet50(weights="IMAGENET1K_V1" if pretrained else None)
    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features, num_classes)  # replace final layer
    return model

num_classes = train_df["dx"].nunique()  # get number of skin cancer classes
model = get_model(num_classes=num_classes, pretrained=True)
model = model.to(device)

print(model)

# --------- Save Model Architecture ---------
model_arch_path = os.path.join(ARTIFACTS_DIR, "model_arch.pth")
torch.save(model.state_dict(), model_arch_path)
print(f"âœ… Model architecture saved to {model_arch_path}")

# ============================================================
# PART 5: EVALUATION & METRICS
# ============================================================

import json
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# --------- Reload best model weights ---------
model = get_model(num_classes=num_classes, pretrained=False)
model.load_state_dict(torch.load(model_arch_path, map_location=device))
model.to(device)
model.eval()
# --------- Define class names ---------
class_names = sorted(train_df['dx'].unique().tolist())
print("Class names:", class_names)
num_classes = len(class_names)

# --------- Evaluation helper ---------
def evaluate_model(model, dataloader, class_names):
    all_preds = []
    all_labels = []
    model.eval()
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Classification report
    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)
    return all_labels, all_preds, report

# --------- Run evaluation ---------
y_true_val, y_pred_val, val_report = evaluate_model(model, valid_loader, class_names)
y_true_test, y_pred_test, test_report = evaluate_model(model, test_loader, class_names)

# --------- Save metrics ---------
metrics_path = os.path.join(ARTIFACTS_DIR, "metrics.json")
with open(metrics_path, "w") as f:
    json.dump({"val": val_report, "test": test_report}, f, indent=2)
print(f"âœ… Metrics saved at {metrics_path}")

# --------- Confusion Matrix (Test set) ---------
cm = confusion_matrix(y_true_test, y_pred_test)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.title("Confusion Matrix (Test Set)")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.show()

# ============================================
# Part 6 - Visual Explanations (Grad-CAM)
# ============================================
import cv2
import numpy as np
import matplotlib.pyplot as plt
import torch
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image

# --------- Prepare Grad-CAM ---------
target_layer = model.layer4[-1]   # last ResNet layer
cam = GradCAM(model=model, target_layers=[target_layer])  # no use_cuda argument

def visualize_gradcam(img_path, model, cam, class_names):
    """
    Given an image path, run the model and visualize Grad-CAM heatmap
    """
    # Load and preprocess image
    img = cv2.imread(img_path)[:, :, ::-1]   # BGR -> RGB
    img = cv2.resize(img, (224, 224))        # Resize to model input
    rgb_img = np.float32(img) / 255.0

    transform = A.Compose([
        A.Resize(224, 224),
        A.Normalize(mean=(0.485, 0.456, 0.406),
                    std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])
    tensor = transform(image=img)["image"].unsqueeze(0).to(device)

    # Prediction
    output = model(tensor)
    pred_class = torch.argmax(output, dim=1).item()
    pred_label = class_names[pred_class]

    # Apply Grad-CAM
    grayscale_cam = cam(input_tensor=tensor,
                        targets=[ClassifierOutputTarget(pred_class)])
    grayscale_cam = grayscale_cam[0, :]

    visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)

    # Show results
    plt.figure(figsize=(8,4))
    plt.subplot(1,2,1)
    plt.imshow(rgb_img)
    plt.title(f"Original Image\nPredicted: {pred_label}")
    plt.axis("off")

    plt.subplot(1,2,2)
    plt.imshow(visualization)
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")
    plt.show()

# --------- Run on a few samples ---------
sample_images = test_df.sample(3, random_state=42)["image_id"].tolist()
for img_id in sample_images:
    img_path = os.path.join(DATA_DIR, "HAM10000_images_part_1", img_id + ".jpg")
    if not os.path.exists(img_path):
        img_path = os.path.join(DATA_DIR, "HAM10000_images_part_2", img_id + ".jpg")
    visualize_gradcam(img_path, model, cam, class_names)

# ============================================
# Part 6 - Visual Explanations (Grad-CAM)
# ============================================
import cv2
import numpy as np
import matplotlib.pyplot as plt
import torch
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image

# --------- Prepare Grad-CAM ---------
target_layer = model.layer4[-1]   # last ResNet layer
cam = GradCAM(model=model, target_layers=[target_layer])  # auto-detects CUDA

def visualize_and_save_gradcam(img_path, model, cam, class_names, save_dir, prefix="gradcam"):
    """
    Runs Grad-CAM and saves visualization to ARTIFACTS_DIR
    """
    # Load and preprocess image
    img = cv2.imread(img_path)[:, :, ::-1]   # BGR -> RGB
    img = cv2.resize(img, (224, 224))
    rgb_img = np.float32(img) / 255.0

    transform = A.Compose([
        A.Resize(224, 224),
        A.Normalize(mean=(0.485, 0.456, 0.406),
                    std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])
    tensor = transform(image=img)["image"].unsqueeze(0).to(device)

    # Prediction
    output = model(tensor)
    pred_class = torch.argmax(output, dim=1).item()
    pred_label = class_names[pred_class]

    # Apply Grad-CAM
    grayscale_cam = cam(input_tensor=tensor, targets=[ClassifierOutputTarget(pred_class)])
    grayscale_cam = grayscale_cam[0, :]
    visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)

    # Save visualization
    save_path = os.path.join(save_dir, f"{prefix}_{os.path.basename(img_path)}")
    cv2.imwrite(save_path, cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))

    # Show inline
    plt.figure(figsize=(8,4))
    plt.subplot(1,2,1)
    plt.imshow(rgb_img)
    plt.title(f"Original\nPred: {pred_label}")
    plt.axis("off")

    plt.subplot(1,2,2)
    plt.imshow(visualization)
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")
    plt.show()

    print(f"âœ… Grad-CAM saved at: {save_path}")


# --------- Run on a few samples ---------
sample_images = test_df.sample(3, random_state=42)["image_id"].tolist()
save_dir = os.path.join(ARTIFACTS_DIR, "gradcam_results")
os.makedirs(save_dir, exist_ok=True)

for img_id in sample_images:
    img_path = os.path.join(DATA_DIR, "HAM10000_images_part_1", img_id + ".jpg")
    if not os.path.exists(img_path):
        img_path = os.path.join(DATA_DIR, "HAM10000_images_part_2", img_id + ".jpg")
    visualize_and_save_gradcam(img_path, model, cam, class_names, save_dir)

# --------- Compute probabilities for ROC (without touching Part 5) ---------
def get_probabilities(model, loader):
    model.eval()
    y_true, y_pred_proba = [], []
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            probs = torch.softmax(outputs, dim=1)  # probabilities
            y_true.extend(labels.cpu().numpy())
            y_pred_proba.extend(probs.cpu().numpy())
    return np.array(y_true), np.array(y_pred_proba)

# Recompute probabilities for validation and test
y_true_val, y_pred_proba_val = get_probabilities(model, valid_loader)
y_true_test, y_pred_proba_test = get_probabilities(model, test_loader)

# --------- Continue Part 7 ---------
plot_and_save_confusion_matrix(y_true_val, y_pred_val, class_names,
                               os.path.join(results_dir, "confusion_matrix_val.png"))
plot_and_save_roc(y_true_val, y_pred_proba_val, class_names,
                  os.path.join(results_dir, "roc_curve_val.png"))

plot_and_save_confusion_matrix(y_true_test, y_pred_test, class_names,
                               os.path.join(results_dir, "confusion_matrix_test.png"))
plot_and_save_roc(y_true_test, y_pred_proba_test, class_names,
                  os.path.join(results_dir, "roc_curve_test.png"))

print("âœ… Confusion matrices + ROC curves saved in:", results_dir)

# ================================
# Part 8: Save Reports & Summary (patched)
# ================================

import pandas as pd
import json

# --------- Save classification reports ---------
def save_classification_report(report, out_path):
    """Save sklearn classification report dict to CSV + TXT."""
    if isinstance(report, dict):
        df = pd.DataFrame(report).transpose()
        df.to_csv(out_path.replace(".txt", ".csv"))
        with open(out_path, "w") as f:
            f.write(str(report))
    else:
        with open(out_path, "w") as f:
            f.write(report)

# Save validation + test reports
save_classification_report(val_report, os.path.join(results_dir, "val_report.txt"))
save_classification_report(test_report, os.path.join(results_dir, "test_report.txt"))

print("âœ… Classification reports saved.")

# --------- Safe defaults if not defined ---------
try:
    lr_value = LEARNING_RATE
except NameError:
    lr_value = 1e-4   # same as what we used in training

try:
    epochs_value = EPOCHS
except NameError:
    epochs_value = "unknown"

# --------- Save summary JSON ---------
summary = {
    "artifact_dir": ARTIFACTS_DIR,
    "dataset": "HAM10000",
    "model": "ResNet18 + transfer learning",
    "img_size": IMG_SIZE,
    "batch_size": BATCH_SIZE,
    "epochs": epochs_value,
    "optimizer": "Adam",
    "learning_rate": lr_value,
    "scheduler": "StepLR",
    "classes": class_names,
    "val_report": val_report,
    "test_report": test_report,
}

summary_path = os.path.join(results_dir, "summary.json")
with open(summary_path, "w") as f:
    json.dump(summary, f, indent=4)

print("âœ… Summary JSON saved at:", summary_path)

# ================================
# Part 9: Generate PDF Report
# ================================
!pip install reportlab

from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.lib.units import inch
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet
import matplotlib.pyplot as plt

# --------- Compute accuracies from reports ---------
def extract_accuracy(report):
    if isinstance(report, dict) and "accuracy" in report:
        return round(report["accuracy"] * 100, 2)
    return None

val_acc = extract_accuracy(val_report)
test_acc = extract_accuracy(test_report)

# --------- PDF Setup ---------
pdf_path = os.path.join(results_dir, "final_report.pdf")
doc = SimpleDocTemplate(pdf_path, pagesize=A4)
styles = getSampleStyleSheet()
story = []

# --------- Add Title ---------
story.append(Paragraph("<b>Skin Lesion Classification Report</b>", styles["Title"]))
story.append(Spacer(1, 0.2*inch))

# --------- Experiment Summary ---------
summary_text = f"""
<b>Experiment Summary</b><br/>
Artifact Directory: {ARTIFACTS_DIR}<br/>
Dataset: HAM10000<br/>
Model: ResNet18 + Transfer Learning<br/>
Image Size: {IMG_SIZE}<br/>
Batch Size: {BATCH_SIZE}<br/>
Epochs: {EPOCHS}<br/>
Optimizer: Adam<br/>
Learning Rate: {lr_value}<br/>
Scheduler: StepLR<br/>
Classes: {", ".join(class_names)}
"""
story.append(Paragraph(summary_text, styles["Normal"]))
story.append(Spacer(1, 0.3*inch))

# --------- Results ---------
results_text = f"""
<b>Evaluation Results</b><br/>
Validation Accuracy: {val_acc}%<br/>
Test Accuracy: {test_acc}%<br/>
"""
story.append(Paragraph(results_text, styles["Normal"]))
story.append(Spacer(1, 0.3*inch))

# --------- Add Images (Confusion Matrix & ROC) ---------
cm_val_path = os.path.join(results_dir, "confusion_matrix_val.png")
roc_val_path = os.path.join(results_dir, "roc_curve_val.png")

for img_path in [cm_val_path, roc_val_path]:
    if os.path.exists(img_path):
        story.append(Image(img_path, width=5*inch, height=4*inch))
        story.append(Spacer(1, 0.3*inch))

# --------- Save PDF ---------
doc.build(story)
print(f"âœ… Final PDF report saved at: {pdf_path}")

import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

# Helper to compute accuracy for a given loader
def compute_accuracy(model, loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return 100 * correct / total

# Assuming 1 epoch = full dataset pass; simulate for visualization
# (This won't give per-epoch loss but shows overall val/test accuracy)
train_acc = compute_accuracy(model, train_loader)
val_acc   = compute_accuracy(model, valid_loader)
test_acc  = compute_accuracy(model, test_loader)

# Plot bar chart for accuracy
plt.figure(figsize=(6,4))
plt.bar(["Train", "Validation", "Test"], [train_acc, val_acc, test_acc], color=['skyblue','orange','green'])
plt.ylabel("Accuracy (%)")
plt.title("Model Accuracy Overview")
plt.ylim(0, 100)
plt.show()